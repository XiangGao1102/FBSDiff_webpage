<!DOCTYPE html>
<html>
<head>
	<meta charset="UTF-8">
	<title>FBSDiff-ACMMM2024</title>
    <style type="text/css">
        body{
        	background-color: white;
        }
        .links{
        	text-decoration: none;
        	color: #0066CC;
        }
        .p2{
        	padding-top: 20px;
        	font-size: 25px;
        }
        .p1{
        	text-align:justify;
        	text-justify:inter-ideograph;
        }
		
		.left {
			text-align: left;
			border: 1px dotted black;
			width: 50%;
		}
        a{
        	font-family: Sans-serif;
        }
        p{
        	font-family: Sans-serif;
        }
        ul{
        	font-family: Sans-serif;
        }
    </style>
</head>
<body>
	<div align="center" style="padding-top: 30px;">
	<p style="font-size:35px;">FBSDiff: Plug-and-Play Frequency Band Substitution of Diffusion Features </p>
	<p style="font-size:35px;"> for Highly Controllable Text-Driven Image Translation </p>

	<a href="mailto:gaoxiang1102@pku.edu.cn" class="links">Xiang Gao</a> &nbsp; &nbsp; 
	<a href="mailto:liujiaying@pku.edu.cn" class="links">Jiaying Liu</a>

	<br>
	<p class="para-3"><span class="p1"> Wangxuan Institute of Computer Technology, Peking University</span></p>
	<p class="para-3"><span class="p1"> Accepted by <i>ACMMM 2024.</i></span></p>
	
	
	</div>
	
	
	<div style="padding-left: 15%; padding-right: 15%;">
                <div align="center">
                    <img src="imgs/teaser.jpg" width="100%"> <br>
                </div>
            <p class='p1'>Based on the pre-trained text-to-image diffusion model, FBSDiff enables efficient text-driven image-to-image translation by proposing a plug-and-play reference image guidance mechanism. It allows flexible control over different guiding factors (e.g., image appearance, image layout, image contours) of the reference image to the T2I generated image, simply by dynamically substituting different types of DCT frequency bands during the reverse sampling process of the diffusion model.</p>
	    </div>
	   
	<div style="padding-left: 15%; padding-right: 15%;">
            <p class='p2'> Resources </p> 
	    <p class='p1'>
		<ul style="line-height:15px">
		　　<li> <a href="" class="links"><a href="https://arxiv.org/pdf/2408.00998">arXiv</a> </li>
		　　<li> <a href="https://github.com/XiangGao1102/FBSDiff" class="links">Code</a> </li>
		</ul>
	</p>
	</div>
	    

        <div align="left" style="padding-left: 15%; padding-right: 15%; padding-bottom: 30px;">
		<p class='p2'> Abstract </p> 
		<p class='p1'>Large-scale text-to-image diffusion models have been a revolutionary milestone in the evolution of generative AI and multimodal technology, allowing wonderful image generation with natural-language text prompt. However, the issue of lacking controllability of such models restricts their practical applicability for real-life content creation. Thus, attention has been focused on leveraging a reference image to control text-to-image synthesis, which is also regarded as manipulating (or editing) a reference image as per a text prompt, namely, text-driven image-to-image translation. This paper contributes a novel, concise, and efficient approach that adapts pre-trained large-scale text-to-image (T2I) diffusion model to the image-to-image (I2I) paradigm in a plug-and-play manner, realizing high-quality and versatile text-driven I2I translation without any model training, model fine-tuning, or online optimization process. To guide T2I generation with a reference image, we propose to decompose diverse guiding factors with different frequency bands of diffusion features in the DCT spectral space, and accordingly devise a novel frequency band substitution layer which realizes dynamic control of the reference image to the T2I generation result in a plug-and-play manner. We demonstrate that our method allows flexible control over both guiding factor and guiding intensity of the reference image simply by tuning the type and bandwidth of the substituted frequency band, respectively. Extensive qualitative and quantitative experiments verify superiority of our approach over related methods in I2I translation visual quality, versatility, and controllability. The code is publicly available at: https://github.com/XiangGao1102/FBSDiff.
		</p>
			<br>
			<p class='p2'> Contributions</p> 
			(1) We provide new insights about controllable diffusion process from a novel frequency-domain perspective.
			<br>
			(2) We propose a novel frequency band substitution technique, realizing plug-and-play text-driven I2I translation without any model training, model fine-tuning, and online optimization process.
			<br>
			(3) We contribute a concise and efficient text-driven I2I framework that is free from source text and cumbersome attention modulations, highly controllable in both guiding factor and guiding intensity of the reference image, and invariant to the architecture of the used diffusion model backbone, all while achieving superior I2I translation performance compared with existing advanced methods.
            <div style="padding-left: 4%; padding-right: 4%;">
                <div align="center">
                    <img src="img/arch.jpg" width="100%"> <br>
                </div>
            <p style="line-height:180%">Figure 1. The overall architecture of FCDiffusion, as well as details of important modules and operations. FCDiffusion comprises the pretrained LDM, a Frequency Filtering Module (FFM), and a FreqControlNet (FCNet). The FFM applies DCT filtering to the source image features, extracting the filtered image features carrying a specific DCT frequency band as control signal, which controls the denoising process of LDM through the FCNet. FCDiffusion integrates multiple control branches with different DCT filters in the FFM, these DCT filters extract different DCT frequency bands to control different I2I correlations (e.g., image style, structure, layout, contour, etc.).
	    </p>
	    </div>
        
            <p class='p2'> Results </p> 
            <div style="padding-left: 4%; padding-right: 4%;">
                <div align="center">
                    <img src="img/style_guided_content_creation.jpg" width="90%"> <br>
                    <p style="line-height:150%">
					Figure 2. Results of style-guided content creation realized with mini-frequency control. The image content is recreated according to the text prompt while the style of the translated image is transferred from the source image.
					</p>
                </div>
			<br/><br/><br/><br/>
		    

                <div align="center">
                    <img src="img/image semantic manipulation.jpg" width="90%"> <br>
                    <p style="line-height:150%">
					Figure 3. Results of image semantic manipulation realized with low-frequency control. The semantics of the source image is manipulated according to the text prompt while the image style and spatial structure are maintained.
					</p>
                </div>

		         <br/><br/><br/><br/>

                <div align="center">
                    <img src="img/image style translation.jpg" width="90%"> <br>
                    <p style="line-height:150%">
					Figure 4. Results of image style translation realized with high-frequency control. The image style (appearance) is modified as per the text prompt while the main contours of the source image are preserved.
					</p>
                </div>

		         <br/><br/><br/><br/>

                <div align="center">
                    <img src="img/image scene translation.jpg" width="90%"> <br>
                    <p style="line-height:150%">
					Figure 5. Results of image scene translation realized with mid-frequency control. The image scene is translated according to the text prompt. In this scenario, the layout of the source image is preserved while the lower-frequency image style and higher-frequency image contours are not restricted.
					</p>
                </div>
	    </div>

		         <br/><br/><br/><br/>
		
	
	<!--<p class='p2'> Citation</p>
	<p> 
		@inproceedings{gao2024frequency, <br>
  			&nbsp; &nbsp; title={Frequency-Controlled Diffusion Model for Versatile Text-Guided Image-to-Image Translation}, <br>
  			&nbsp; &nbsp; author={Gao, Xiang and Xu, Zhengbo and Zhao, Junhan and Liu, Jiaying}, <br>
  			&nbsp; &nbsp; booktitle={Proceedings of the AAAI Conference on Artificial Intelligence}, <br>
  			&nbsp; &nbsp; volume={38}, <br>
  			&nbsp; &nbsp; number={3}, <br>
  			&nbsp; &nbsp; pages={1824--1832}, <br>
  			&nbsp; &nbsp; year={2024} <br>
		}
	</p>	-->

</html>
